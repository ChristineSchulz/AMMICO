<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AMMICO - AI Media and Misinformation Content Analysis Tool &mdash; AMMICO 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=d45e8c67"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="AMMICO: AI Media and Misinformation Content Analysis Tool" href="notebooks/DemoNotebook_ammico.html" />
    <link rel="prev" title="Welcome to AMMICO’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            AMMICO
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">AMMICO - AI Media and Misinformation Content Analysis Tool</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compatibility-problems-solving">Compatibility problems solving</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#first-install-tensorflow-https-www-tensorflow-org-install-pip">1. First, install tensorflow (https://www.tensorflow.org/install/pip)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#second-install-pytorch">2. Second, install pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#after-we-prepared-right-environment-we-can-install-the-ammico-package">3. After we prepared right environment we can install the <code class="docutils literal notranslate"><span class="pre">ammico</span></code> package</a></li>
<li class="toctree-l3"><a class="reference internal" href="#micromamba">Micromamba</a></li>
<li class="toctree-l3"><a class="reference internal" href="#windows">Windows</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#features">Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#text-extraction">Text extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#content-extraction">Content extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#emotion-recognition">Emotion recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#color-hue-detection">Color/hue detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cropping-of-posts">Cropping of posts</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DemoNotebook_ammico.html">AMMICO: AI Media and Misinformation Content Analysis Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DemoNotebook_ammico.html#Step-1:-Read-your-data-into-AMMICO">Step 1: Read your data into AMMICO</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DemoNotebook_ammico.html#The-detector-modules">The detector modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/Example%20multimodal.html">Image Multimodal Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/Example%20colors.html">Color analysis of pictures</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/Example%20cropposts.html">Crop posts from social media posts images</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">AMMICO package modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="license_link.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMMICO</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">AMMICO - AI Media and Misinformation Content Analysis Tool</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/readme_link.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ammico-ai-media-and-misinformation-content-analysis-tool">
<h1>AMMICO - AI Media and Misinformation Content Analysis Tool<a class="headerlink" href="#ammico-ai-media-and-misinformation-content-analysis-tool" title="Link to this heading"></a></h1>
<p><img alt="License: MIT" src="https://img.shields.io/github/license/ssciwr/AMMICO" />
<img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/ssciwr/AMMICO/ci.yml?branch=main" />
<img alt="codecov" src="https://img.shields.io/codecov/c/github/ssciwr/AMMICO" />
<img alt="Quality Gate Status" src="https://sonarcloud.io/api/project_badges/measure?project=ssciwr_ammico&amp;metric=alert_status" />
<img alt="Language" src="https://img.shields.io/github/languages/top/ssciwr/AMMICO" /></p>
<p>This package extracts data from images such as social media posts that contain an image part and a text part. The analysis can generate a very large number of features, depending on the user input. See <a class="reference external" href="https://dx.doi.org/10.31235/osf.io/v8txj">our paper</a> for a more in-depth description.</p>
<p><strong><em>This project is currently under development!</em></strong></p>
<p>Use pre-processed image files such as social media posts with comments and process to collect information:</p>
<ol class="arabic simple">
<li><p>Text extraction from the images</p>
<ol class="arabic simple">
<li><p>Language detection</p></li>
<li><p>Translation into English or other languages</p></li>
<li><p>Cleaning of the text, spell-check</p></li>
<li><p>Sentiment analysis</p></li>
<li><p>Named entity recognition</p></li>
<li><p>Topic analysis</p></li>
</ol>
</li>
<li><p>Content extraction from the images</p>
<ol class="arabic simple">
<li><p>Textual summary of the image content (“image caption”) that can be analyzed further using the above tools</p></li>
<li><p>Feature extraction from the images: User inputs query and images are matched to that query (both text and image query)</p></li>
<li><p>Question answering</p></li>
</ol>
</li>
<li><p>Performing person and face recognition in images</p>
<ol class="arabic simple">
<li><p>Face mask detection</p></li>
<li><p>Age, gender and race detection</p></li>
<li><p>Emotion recognition</p></li>
</ol>
</li>
<li><p>Color analysis</p>
<ol class="arabic simple">
<li><p>Analyse hue and percentage of color on image</p></li>
</ol>
</li>
<li><p>Multimodal analysis</p>
<ol class="arabic simple">
<li><p>Find best matches for image content or image similarity</p></li>
</ol>
</li>
<li><p>Cropping images to remove comments from posts</p></li>
</ol>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">AMMICO</span></code> package can be installed using pip:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">ammico</span>
</pre></div>
</div>
<p>This will install the package and its dependencies locally. If after installation you get some errors when running some modules, please follow the instructions below.</p>
</section>
<section id="compatibility-problems-solving">
<h2>Compatibility problems solving<a class="headerlink" href="#compatibility-problems-solving" title="Link to this heading"></a></h2>
<p>Some ammico components require <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> (e.g. Emotion detector), some <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> (e.g. Summary detector). Sometimes there are compatibility problems between these two frameworks. To avoid these problems on your machines, you can prepare proper environment before installing the package (you need conda on your machine):</p>
<section id="first-install-tensorflow-https-www-tensorflow-org-install-pip">
<h3>1. First, install tensorflow (https://www.tensorflow.org/install/pip)<a class="headerlink" href="#first-install-tensorflow-https-www-tensorflow-org-install-pip" title="Link to this heading"></a></h3>
<ul>
<li><p>create a new environment with python and activate it</p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">create</span> <span class="pre">-n</span> <span class="pre">ammico_env</span> <span class="pre">python=3.10</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">ammico_env</span></code></p>
</li>
<li><p>install cudatoolkit from conda-forge</p>
<p><code class="docutils literal notranslate"> <span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">cudatoolkit=11.8.0</span></code></p>
</li>
<li><p>install nvidia-cudnn-cu11 from pip</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">nvidia-cudnn-cu11==8.6.0.163</span></code></p>
</li>
<li><p>add script that runs when conda environment <code class="docutils literal notranslate"><span class="pre">ammico_env</span></code> is activated to put the right libraries on your LD_LIBRARY_PATH</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo &#39;CUDNN_PATH=$(dirname $(python -c &quot;import nvidia.cudnn;print(nvidia.cudnn.__file__)&quot;))&#39; &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo &#39;export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$CONDA_PREFIX/lib/:$LD_LIBRARY_PATH&#39; &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
source $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
</pre></div>
</div>
</li>
<li><p>deactivate and re-activate conda environment to call script above</p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">deactivate</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">ammico_env</span> </code></p>
</li>
<li><p>install tensorflow</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">tensorflow==2.12.1</span></code></p>
</li>
</ul>
</section>
<section id="second-install-pytorch">
<h3>2. Second, install pytorch<a class="headerlink" href="#second-install-pytorch" title="Link to this heading"></a></h3>
<ul>
<li><p>install pytorch for same cuda version as above</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torch</span> <span class="pre">torchvision</span> <span class="pre">torchaudio</span> <span class="pre">--index-url</span> <span class="pre">https://download.pytorch.org/whl/cu118</span></code></p>
</li>
</ul>
</section>
<section id="after-we-prepared-right-environment-we-can-install-the-ammico-package">
<h3>3. After we prepared right environment we can install the <code class="docutils literal notranslate"><span class="pre">ammico</span></code> package<a class="headerlink" href="#after-we-prepared-right-environment-we-can-install-the-ammico-package" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">ammico</span></code></p></li>
</ul>
<p>It is done.</p>
</section>
<section id="micromamba">
<h3>Micromamba<a class="headerlink" href="#micromamba" title="Link to this heading"></a></h3>
<p>If you are using micromamba you can prepare environment with just one command:</p>
<p><code class="docutils literal notranslate"><span class="pre">micromamba</span> <span class="pre">create</span> <span class="pre">--no-channel-priority</span> <span class="pre">-c</span> <span class="pre">nvidia</span> <span class="pre">-c</span> <span class="pre">pytorch</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">-n</span> <span class="pre">ammico_env</span> <span class="pre">&quot;python=3.10&quot;</span> <span class="pre">pytorch</span> <span class="pre">torchvision</span> <span class="pre">torchaudio</span> <span class="pre">pytorch-cuda</span> <span class="pre">&quot;tensorflow-gpu&lt;=2.12.3&quot;</span> <span class="pre">&quot;numpy&lt;=1.23.4&quot;</span></code></p>
</section>
<section id="windows">
<h3>Windows<a class="headerlink" href="#windows" title="Link to this heading"></a></h3>
<p>To make pycocotools work on Windows OS you may need to install <code class="docutils literal notranslate"><span class="pre">vs_BuildTools.exe</span></code> from https://visualstudio.microsoft.com/visual-cpp-build-tools/ and choose following elements:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Visual</span> <span class="pre">Studio</span> <span class="pre">extension</span> <span class="pre">development</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MSVC</span> <span class="pre">v143</span> <span class="pre">-</span> <span class="pre">VS</span> <span class="pre">2022</span> <span class="pre">C++</span> <span class="pre">x64/x86</span> <span class="pre">build</span> <span class="pre">tools</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Windows</span> <span class="pre">11</span> <span class="pre">SDK</span></code> for Windows 11 (or <code class="docutils literal notranslate"><span class="pre">Windows</span> <span class="pre">10</span> <span class="pre">SDK</span></code> for Windows 10)</p></li>
</ul>
<p>Be careful, it requires around 7 GB of disk space.</p>
<p><img alt="Screenshot 2023-06-01 165712" src="https://github.com/ssciwr/AMMICO/assets/8105097/3dfb302f-c390-46a7-a700-4e044f56c30f" /></p>
</section>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h2>
<p>The main demonstration notebook can be found in the <code class="docutils literal notranslate"><span class="pre">notebooks</span></code> folder and also on <a class="reference external" href="https://colab.research.google.com/github/ssciwr/ammico/blob/main/ammico/notebooks/DemoNotebook_ammico.ipynb">google colab</a></p>
<p>There are further sample notebooks in the <code class="docutils literal notranslate"><span class="pre">notebooks</span></code> folder for the more experimental features:</p>
<ol class="arabic simple">
<li><p>Topic analysis: Use the notebook <code class="docutils literal notranslate"><span class="pre">get-text-from-image.ipynb</span></code> to analyse the topics of the extraced text.<br />
<strong>You can run this notebook on google colab: <a class="reference external" href="https://colab.research.google.com/github/ssciwr/ammico/blob/main/ammico/notebooks/get-text-from-image.ipynb">Here</a></strong><br />
Place the data files and google cloud vision API key in your google drive to access the data.</p></li>
<li><p>Multimodal content: Use the notebook <code class="docutils literal notranslate"><span class="pre">multimodal_search.ipynb</span></code> to find the best fitting images to an image or text query.
<strong>You can run this notebook on google colab: <a class="reference external" href="https://colab.research.google.com/github/ssciwr/ammico/blob/main/ammico/notebooks/multimodal_search.ipynb">Here</a></strong></p></li>
<li><p>Color analysis: Use the notebook <code class="docutils literal notranslate"><span class="pre">color_analysis.ipynb</span></code> to identify colors the image. The colors are then classified into the main named colors in the English language.
<strong>You can run this notebook on google colab: <a class="reference external" href="https://colab.research.google.com/github/ssciwr/ammico/blob/main/ammico/notebooks/colors_analysis.ipynb">Here</a></strong></p></li>
<li><p>To crop social media posts use the <code class="docutils literal notranslate"><span class="pre">cropposts.ipynb</span></code> notebook.
<strong>You can run this notebook on google colab: <a class="reference external" href="https://colab.research.google.com/github/ssciwr/ammico/blob/main/ammico/notebooks/cropposts.ipynb">Here</a></strong></p></li>
</ol>
</section>
<section id="features">
<h2>Features<a class="headerlink" href="#features" title="Link to this heading"></a></h2>
<section id="text-extraction">
<h3>Text extraction<a class="headerlink" href="#text-extraction" title="Link to this heading"></a></h3>
<p>The text is extracted from the images using <a class="reference external" href="https://cloud.google.com/vision">google-cloud-vision</a>. For this, you need an API key. Set up your google account following the instructions on the google Vision AI website.
You then need to export the location of the API key as an environment variable:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">GOOGLE_APPLICATION_CREDENTIALS</span><span class="o">=</span><span class="s2">&quot;location of your .json&quot;</span>
</pre></div>
</div>
<p>The extracted text is then stored under the <code class="docutils literal notranslate"><span class="pre">text</span></code> key (column when exporting a csv).</p>
<p><a class="reference external" href="https://py-googletrans.readthedocs.io/en/latest/">Googletrans</a> is used to recognize the language automatically and translate into English. The text language and translated text is then stored under the <code class="docutils literal notranslate"><span class="pre">text_language</span></code> and <code class="docutils literal notranslate"><span class="pre">text_english</span></code> key (column when exporting a csv).</p>
<p>If you further want to analyse the text, you have to set the <code class="docutils literal notranslate"><span class="pre">analyse_text</span></code> keyword to <code class="docutils literal notranslate"><span class="pre">True</span></code>. In doing so, the text is then processed using <a class="reference external" href="https://spacy.io/">spacy</a> (tokenized, part-of-speech, lemma, …). The English text is cleaned from numbers and unrecognized words (<code class="docutils literal notranslate"><span class="pre">text_clean</span></code>), spelling of the English text is corrected (<code class="docutils literal notranslate"><span class="pre">text_english_correct</span></code>), and further sentiment and subjectivity analysis are carried out (<code class="docutils literal notranslate"><span class="pre">polarity</span></code>, <code class="docutils literal notranslate"><span class="pre">subjectivity</span></code>). The latter two steps are carried out using <a class="reference external" href="https://textblob.readthedocs.io/en/dev/index.html">TextBlob</a>. For more information on the sentiment analysis using TextBlob see <a class="reference external" href="https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524">here</a>.</p>
<p>The <a class="reference external" href="https://huggingface.co/">Hugging Face transformers library</a> is used to perform another sentiment analysis, a text summary, and named entity recognition, using the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> pipeline.</p>
</section>
<section id="content-extraction">
<h3>Content extraction<a class="headerlink" href="#content-extraction" title="Link to this heading"></a></h3>
<p>The image content (“caption”) is extracted using the <a class="reference external" href="https://github.com/salesforce/LAVIS">LAVIS</a> library. This library enables vision intelligence extraction using several state-of-the-art models, depending on the task. Further, it allows feature extraction from the images, where users can input textual and image queries, and the images in the database are matched to that query (multimodal search). Another option is question answering, where the user inputs a text question and the library finds the images that match the query.</p>
</section>
<section id="emotion-recognition">
<h3>Emotion recognition<a class="headerlink" href="#emotion-recognition" title="Link to this heading"></a></h3>
<p>Emotion recognition is carried out using the <a class="reference external" href="https://github.com/serengil/deepface">deepface</a> and <a class="reference external" href="https://github.com/serengil/retinaface">retinaface</a> libraries. These libraries detect the presence of faces, and their age, gender, emotion and race based on several state-of-the-art models. It is also detected if the person is wearing a face mask - if they are, then no further detection is carried out as the mask prevents an accurate prediction.</p>
</section>
<section id="color-hue-detection">
<h3>Color/hue detection<a class="headerlink" href="#color-hue-detection" title="Link to this heading"></a></h3>
<p>Color detection is carried out using <a class="reference external" href="https://github.com/obskyr/colorgram.py">colorgram.py</a> and <a class="reference external" href="https://github.com/vaab/colour">colour</a> for the distance metric. The colors can be classified into the main named colors/hues in the English language, that are red, green, blue, yellow, cyan, orange, purple, pink, brown, grey, white, black.</p>
</section>
<section id="cropping-of-posts">
<h3>Cropping of posts<a class="headerlink" href="#cropping-of-posts" title="Link to this heading"></a></h3>
<p>Social media posts can automatically be cropped to remove further comments on the page and restrict the textual content to the first comment only.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to AMMICO’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="notebooks/DemoNotebook_ammico.html" class="btn btn-neutral float-right" title="AMMICO: AI Media and Misinformation Content Analysis Tool" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Scientific Software Center, Heidelberg University.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>